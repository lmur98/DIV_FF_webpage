<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DIV-FF - CVPR 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>
<body>
    <header>
        <h1>ğŸš€ DIV-FF ğŸš€ </h1>
        <h2>Dynamic Image-Video Feature Fields
            For Environment Understanding in Egocentric Videos
        </h2>
        <h2>âœ¨âœ¨ Accepted at CVPR 2025 âœ¨âœ¨</h2>
    </header>

    <section id="video-slider">
        <h2>Visualize our results!</h2>
        <p>Click the button to change the scene. DIV-FF renders objects and affordances in egocentric videos.</p>
        <div class="video-container">
            <video id="videoDisplay" controls>
                <source src="video1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="slider-container">
            <button id="prevBtn">&#9665;</button>
            <button id="nextBtn">&#9655;</button>
        </div>
        
    </section>
    
    <section id="abstract">
        <h2>Abstract</h2>
        <p>Environment understanding in egocentric videos is an important step for applications like robotics, augmented reality and assistive technologies. These videos are characterized 
            by dynamic interactions and a strong dependence on the wearer engagement with the environment. Traditional approaches often focus on isolated clips or fail to integrate rich semantic 
            and geometric information, limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields (DIV-FF), a framework that decomposes the egocentric scene into persistent, 
            dynamic, and actor-based components while integrating both image and video-language features. Our model enables detailed segmentation, captures affordances, understands the surroundings
            and maintains consistent understanding over time. DIV-FF outperforms state-of-the-art methods, particularly in dynamically evolving scenarios, demonstrating its potential to advance 
            long-term, spatio-temporal scene understanding.
        </p>
        <p> ğŸ“„ğŸ“„
            <a href="https://arxiv.org/abs/example" target="_blank">Read the Paper</a>
            ğŸ“„ğŸ“„
        </p>
        
    </section>

    <section id="model"> 
        <h2>Model architecture</h2>
        <p>Overview of DIV-FF. Our three-stream architecture field predicts the color \( c \), the density \( \sigma \), the material aleatoric uncertainty \( \beta \),
            the image-language features \( \phi \) and the video-language features \( \psi \) along a ray \( r \) with direction \( d \) given the camera viewpoint \( g \) and a frame
            specific code \( z \). We first extract SAM masks and bounding boxes from the image, that we leverage to obtain a unique CLIP descriptor \( \psi (I^{GT}) \)
            in all the pixels within the respective mask. We supervise the video-language feature field with local patch features and a global
            video embedding assigned only to pixels in the interaction hotspot, computed with a pre-trained hand-object detector.
        </p>
        <div class="image-container">
            <img src="images/model.png" alt="Model" class="responsive-image">
        </div>
    </section>
    
    <section id="results">
        <h2>Affordance Segmentation</h2>
        <p>DIV-FF distills Ego-Video features for enabling affordance segmentation in egocentric videos.
           While image-language features (CLIP) perform well when actions are explicitly linked to objects, such as
           â€œcut the onionâ€ or â€œadd ingredients to the mixtureâ€, our distilled video-language features accurately identifies the action's interaction hotspot.
        </p>
        <div class="image-container">
            <img src="images/aff_qualit.png" alt="Qualitative results" class="responsive-image">
        </div>
    </section>

    <section id="results2">
        <h2>Dynamic Object Segmentation</h2>
        <p>DIV-FF also shows consistent Dynamic Object Segmentation along different time-steps in novel views: The dynamic and actor streams contain
            respective frame-specific codes \( z^f_t \) and \( z^a_t \). This time encoding is also propagated to the semantic feature field, obtaining consistent segmentations despite the continuous movement of the â€œspatulaâ€ and â€œblue cutting boardâ€
        </p>
        <div class="image-container">
            <img src="images/dyn.png" alt="Dyn Qualitative results" class="responsive-image">
        </div>
    </section>
    
    <section id="code">
        <h2>Code & Dataset</h2>
        <p>ğŸ’»ğŸ’» Access the GitHub <a href="https://github.com/lmur98/FEATURE_FIELDS_CVPR">code and weights</a> ğŸ’»ğŸ’»</p>
    </section>
    
    <section id="contact">
        <h2>Contact</h2>
        <p>For questions, reach out at ğŸ“©ğŸ“© <a href="mailto:lmur@unizar.es">e-mail</a> ğŸ“©ğŸ“©
           or chek my  <a href="https://sites.google.com/unizar.es/lorenzo-mur-labadia/inicio">personal web-page</a>
        </p>
    </section>
    
    <footer>
        <p>&copy; 2025 Your Name | Hosted on GitHub Pages</p>
    </footer>

    <script>
        const videos = ["videos/P03_04.mp4", "videos/P04_01.mp4", "videos/P13_03.mp4"]; // Add your MP4 files here
        let currentIndex = 0;
        const videoDisplay = document.getElementById("videoDisplay");
        
        document.getElementById("prevBtn").addEventListener("click", () => {
            currentIndex = (currentIndex - 1 + videos.length) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
        
        document.getElementById("nextBtn").addEventListener("click", () => {
            currentIndex = (currentIndex + 1) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
    </script>
</body>
</html>
