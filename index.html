<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DIV-FF - CVPR 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <h1>DIV-FF</h1>
        <h2>Dynamic Image-Video Feature Fields
            For Environment Understanding in Egocentric Videos
        </h2>
        <p>Accepted at CVPR 2025</p>
    </header>

    <section id="video-slider">
        <h2>Visualize our results!</h2>
        <div class="slider-container">
            <button id="prevBtn">&#9665;</button>
            <button id="nextBtn">&#9655;</button>
        </div>
        <video id="videoDisplay" width="1140" height="315" controls>
            <source src="video1.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>
    
    <section id="abstract">
        <h2>Abstract</h2>
        <p>Environment understanding in egocentric videos is an important step for applications like robotics, augmented reality and assistive technologies. These videos are characterized 
            by dynamic interactions and a strong dependence on the wearer engagement with the environment. Traditional approaches often focus on isolated clips or fail to integrate rich semantic 
            and geometric information, limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields (DIV-FF), a framework that decomposes the egocentric scene into persistent, 
            dynamic, and actor-based components while integrating both image and video-language features. Our model enables detailed segmentation, captures affordances, understands the surroundings
            and maintains consistent understanding over time. DIV-FF outperforms state-of-the-art methods, particularly in dynamically evolving scenarios, demonstrating its potential to advance 
            long-term, spatio-temporal scene understanding.
        </p>
        <a href="https://arxiv.org/abs/example" target="_blank">Read the Paper</a>
    </section>
    
    <section id="demo">
        <h2>Demo</h2>
        <p>Interactive demo or video.</p>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/example" frameborder="0" allowfullscreen></iframe>
    </section>
    
    <section id="code">
        <h2>Code & Dataset</h2>
        <p>Access the implementation and dataset:</p>
        <ul>
            <li><a href="https://github.com/your-repo" target="_blank">GitHub Repository</a></li>
            <li><a href="https://huggingface.co/your-model" target="_blank">Dataset/Model</a></li>
        </ul>
    </section>
    
    <section id="contact">
        <h2>Contact</h2>
        <p>For questions, reach out at: <a href="mailto:your.email@example.com">your.email@example.com</a></p>
    </section>
    
    <footer>
        <p>&copy; 2025 Your Name | Hosted on GitHub Pages</p>
    </footer>

    <script>
        const videos = ["videos/P03_04.mp4", "videos/P04_01.mp4", "videos/P13_03.mp4"]; // Add your MP4 files here
        let currentIndex = 0;
        const videoDisplay = document.getElementById("videoDisplay");
        
        document.getElementById("prevBtn").addEventListener("click", () => {
            currentIndex = (currentIndex - 1 + videos.length) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
        
        document.getElementById("nextBtn").addEventListener("click", () => {
            currentIndex = (currentIndex + 1) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
    </script>
</body>
</html>
