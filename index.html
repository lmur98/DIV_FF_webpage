<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DIV-FF - CVPR 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>
<body>
    <header>
        <h1>ðŸš€ DIV-FF ðŸš€ </h1>
        <h2>Dynamic Image-Video Feature Fields
            For Environment Understanding in Egocentric Videos
        </h2>
        <h2>âœ¨âœ¨ Accepted at CVPR 2025 âœ¨âœ¨</h2>
    </header>

    <section id="video-slider">
        <h2>Visualize our results!</h2>
        <p>Click the button to change the scene. DIV-FF renders objects and affordances in egocentric videos.</p>
        <div class="video-container">
            <video id="videoDisplay" controls>
                <source src="video1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="slider-container">
            <button id="prevBtn">&#9665;</button>
            <button id="nextBtn">&#9655;</button>
        </div>
        
    </section>
    
    <section id="abstract">
        <h2>Abstract</h2>
        <p>Environment understanding in egocentric videos is an important step for applications like robotics, augmented reality and assistive technologies. These videos are characterized 
            by dynamic interactions and a strong dependence on the wearer engagement with the environment. Traditional approaches often focus on isolated clips or fail to integrate rich semantic 
            and geometric information, limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields (DIV-FF), a framework that decomposes the egocentric scene into persistent, 
            dynamic, and actor-based components while integrating both image and video-language features. Our model enables detailed segmentation, captures affordances, understands the surroundings
            and maintains consistent understanding over time. DIV-FF outperforms state-of-the-art methods, particularly in dynamically evolving scenarios, demonstrating its potential to advance 
            long-term, spatio-temporal scene understanding.
        </p>
        <a href="https://arxiv.org/abs/example" target="_blank">Read the Paper</a>
    </section>

    <section id="model"> 
        <h2>Model architecture</h2>
        <p>Overview of DIV-FF. Our three-stream architecture field predicts the color \( c \), the density \( \sigma \), the material aleatoric uncertainty \( \beta \),
            the image-language features \( \phi \) and the video-language features \psi along a ray \( r \) with direction \( d \) given the camera viewpoint g and a frame
            specific code z. We first extract SAM masks and bounding boxes from the image, that we leverage to obtain a unique CLIP descriptor Ï•GT
            in all the pixels within the respective mask. We supervise the video-language feature field with local patch features Ïˆ GT (Vp ) and a global
            video embedding Ïˆ GT (V ) assigned only to pixels in the interaction hotspot MIH , computed with a pre-trained hand-object detector.
        </p>
        <div class="image-container">
            <img src="images/aff_qualit.png" alt="Qualitative results" class="responsive-image">
        </div>
    </section>
    
    <section id="results">
        <h2>Affordance segmentation</h2>
        <p>DIV-FF distills Ego-Video features for enabling affordance segmentation in egocentric videos.
           Compared with using CLIP features, DIV-FF captures complex interactions
        </p>
        <div class="image-container">
            <img src="images/aff_qualit.png" alt="Qualitative results" class="responsive-image">
        </div>
    </section>
    
    <section id="code">
        <h2>Code & Dataset</h2>
        <p>Access the implementation and dataset:</p>
        <ul>
            <li><a href="https://github.com/your-repo" target="_blank">GitHub Repository</a></li>
            <li><a href="https://huggingface.co/your-model" target="_blank">Dataset/Model</a></li>
        </ul>
    </section>
    
    <section id="contact">
        <h2>Contact</h2>
        <p>For questions, reach out at: <a href="mailto:lmur@unizar.es">e-mail</a> 
           or chek my  <a href="https://sites.google.com/unizar.es/lorenzo-mur-labadia/inicio">personal web-page</a>
        </p>
    </section>
    
    <footer>
        <p>&copy; 2025 Your Name | Hosted on GitHub Pages</p>
    </footer>

    <script>
        const videos = ["videos/P03_04.mp4", "videos/P04_01.mp4", "videos/P13_03.mp4"]; // Add your MP4 files here
        let currentIndex = 0;
        const videoDisplay = document.getElementById("videoDisplay");
        
        document.getElementById("prevBtn").addEventListener("click", () => {
            currentIndex = (currentIndex - 1 + videos.length) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
        
        document.getElementById("nextBtn").addEventListener("click", () => {
            currentIndex = (currentIndex + 1) % videos.length;
            videoDisplay.src = videos[currentIndex];
            videoDisplay.play();
        });
    </script>
</body>
</html>
